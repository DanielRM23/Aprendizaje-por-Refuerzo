import numpy as np 
import random  

    # --------------------------- Evaluaci칩n de pol칤tica ---------------------------

class TicTacToeAI:
    def __init__(self, juego):
        """
        Inicializa la IA con una instancia del juego TicTacToe.
        - juego: Instancia de la clase TicTacToe.
        - estados_valores: Diccionario para almacenar los valores de los estados.
        - politica: Diccionario para almacenar la pol칤tica 칩ptima.
        """
        self.juego = juego  # Guarda la instancia del juego.
        self.estados_valores = {}  # Diccionario para almacenar los valores de los estados.
        self.politica = {}  # Diccionario para almacenar la pol칤tica 칩ptima.

    def inicializar_politica(self):
        """
        Inicializa la pol칤tica con acciones aleatorias para cada estado.
        Recorre todos los estados en `estados_valores` y asigna una acci칩n aleatoria
        si hay movimientos posibles en ese estado.
        """
        for estado in self.estados_valores:  # Recorre cada estado en el diccionario de valores.
            acciones_posibles = self.juego.posibilidades(np.array(estado))  # Obtiene las acciones posibles para el estado.
            if acciones_posibles:  # Si hay acciones posibles:
                self.politica[estado] = random.choice(acciones_posibles)  # Asigna una acci칩n aleatoria a la pol칤tica.

    def inicializar_valores_estado(self):
        """
        Inicializa los valores de todos los estados a cero.
        """
        for estado in self.estados_valores:  # Recorre cada estado en el diccionario de valores.
            self.estados_valores[estado] = 0  # Inicializa el valor del estado a cero.

    def evaluar_politica(self, gamma=0.9, theta=1e-6):
        """
        Eval칰a la pol칤tica actual calculando los valores de los estados.
        - gamma: Factor de descuento (por defecto 0.9).
        - theta: Umbral de convergencia (por defecto 1e-6).
        """
        while True:  # Bucle infinito hasta que se alcance la convergencia.
            delta = 0  # Inicializa la diferencia m치xima entre iteraciones.
            for estado in self.estados_valores:  # Recorre cada estado en el diccionario de valores.
                v = self.estados_valores[estado]  # Guarda el valor actual del estado.
                accion = self.politica[estado]  # Obtiene la acci칩n seg칰n la pol칤tica actual.
                nuevo_tablero = self.juego.aplicar_accion(np.array(estado), accion, 1)  # Aplica la acci칩n al estado actual.
                nuevo_estado = self.juego.obtener_estado(nuevo_tablero)  # Obtiene el nuevo estado despu칠s de la acci칩n.
                recompensa = self.juego.es_terminal(nuevo_tablero)  # Calcula la recompensa del nuevo estado.

                # Define la recompensa seg칰n el resultado del juego:
                if recompensa == 1:
                    recompensa = 1  # El agente gana.
                elif recompensa == 2:
                    recompensa = -1  # El oponente gana.
                elif recompensa == -1:
                    recompensa = 0  # Empate.
                else:
                    recompensa = 0  # Juego en progreso.

                # Actualiza el valor del estado usando la ecuaci칩n de Bellman:
                self.estados_valores[estado] = recompensa + gamma * self.estados_valores.get(nuevo_estado, 0)
                delta = max(delta, abs(v - self.estados_valores[estado]))  # Actualiza la diferencia m치xima.

            # Si la diferencia entre iteraciones es menor que theta, se detiene:
            if delta < theta:
                break

    def mejorar_politica(self, gamma=0.9):
        """
        Mejora la pol칤tica actual seleccionando la acci칩n que maximiza el valor esperado.
        - gamma: Factor de descuento (por defecto 0.9).
        Retorna True si la pol칤tica es estable, False si cambi칩.
        """
        politica_estable = True  # Inicializa la variable que indica si la pol칤tica es estable.
        for estado in self.estados_valores:  # Recorre cada estado en el diccionario de valores.
            acciones_posibles = self.juego.posibilidades(np.array(estado))  # Obtiene las acciones posibles para el estado.
            if acciones_posibles:  # Si hay acciones posibles:
                mejor_accion = None  # Inicializa la mejor acci칩n.
                mejor_valor = -float('inf')  # Inicializa el mejor valor con un n칰mero muy peque침o.

                # Busca la mejor acci칩n para el estado actual:
                for accion in acciones_posibles:  # Recorre cada acci칩n posible.
                    nuevo_tablero = self.juego.aplicar_accion(np.array(estado), accion, 1)  # Aplica la acci칩n.
                    nuevo_estado = self.juego.obtener_estado(nuevo_tablero)  # Obtiene el nuevo estado.
                    recompensa = self.juego.es_terminal(nuevo_tablero)  # Calcula la recompensa.

                    # Define la recompensa seg칰n el resultado del juego:
                    if recompensa == 1:
                        recompensa = 1
                    elif recompensa == 2:
                        recompensa = -1
                    elif recompensa == -1:
                        recompensa = 0
                    else:
                        recompensa = 0

                    # Calcula el valor esperado:
                    valor = recompensa + gamma * self.estados_valores.get(nuevo_estado, 0)
                    if valor > mejor_valor:  # Si el valor es mejor que el actual:
                        mejor_valor = valor  # Actualiza el mejor valor.
                        mejor_accion = accion  # Actualiza la mejor acci칩n.

                # Actualiza la pol칤tica si es necesario:
                if self.politica[estado] != mejor_accion:  # Si la acci칩n cambi칩:
                    politica_estable = False  # La pol칤tica no es estable.
                    self.politica[estado] = mejor_accion  # Actualiza la pol칤tica.

        return politica_estable  # Retorna si la pol칤tica es estable.

    def iteracion_por_politica(self, gamma=0.9, theta=1e-6):
        """
        Realiza la iteraci칩n por pol칤tica hasta que la pol칤tica sea estable.
        - gamma: Factor de descuento (por defecto 0.9).
        - theta: Umbral de convergencia (por defecto 1e-6).
        """
        self.inicializar_politica()  # Inicializa la pol칤tica.
        self.inicializar_valores_estado()  # Inicializa los valores de los estados.
        while True:  # Bucle infinito hasta que la pol칤tica sea estable.
            self.evaluar_politica(gamma, theta)  # Eval칰a la pol칤tica.
            politica_estable = self.mejorar_politica(gamma)  # Mejora la pol칤tica.
            if politica_estable:  # Si la pol칤tica es estable:
                break  # Termina el bucle.

    def jugar_con_politica(self):
        """
        Juega una partida utilizando la pol칤tica 칩ptima aprendida.
        """
        self.juego.tablero = self.juego.crear_tablero()  # Crea un nuevo tablero.
        turno = 1  # Inicializa el turno del jugador 1.
        while self.juego.es_terminal(self.juego.tablero) == 0:  # Mientras el juego no termine:
            estado = self.juego.obtener_estado(self.juego.tablero)  # Obtiene el estado actual.
            if estado in self.politica:  # Si el estado est치 en la pol칤tica:
                accion = self.politica[estado]  # Usa la acci칩n de la pol칤tica.
            else:  # Si no est치 en la pol칤tica:
                accion = random.choice(self.juego.posibilidades(self.juego.tablero))  # Elige una acci칩n aleatoria.
            self.juego.tablero = self.juego.aplicar_accion(self.juego.tablero, accion, turno)  # Aplica la acci칩n.
            print(f"Turno del jugador {turno}:\n", self.juego.tablero, "\n")  # Muestra el tablero.
            turno = 3 - turno  # Alterna entre el jugador 1 y 2.
        ganador = self.juego.es_terminal(self.juego.tablero)  # Obtiene el ganador.
        print("춰Gana el jugador!" if ganador in [1, 2] else "Empate.")  # Muestra el resultado.

    def mostrar_valores_y_politica(self, max_mostrar=10):
        """
        Muestra los valores 칩ptimos de los estados y la pol칤tica 칩ptima.
        - max_mostrar: N칰mero m치ximo de estados y pol칤ticas a mostrar (por defecto 10).
        """
        print("\n游댳 Valores 칍ptimos de los Estados (V*):")
        for i, (estado, valor) in enumerate(self.estados_valores.items()):  # Recorre los estados y valores.
            print(f"Estado {i+1}: {estado} -> V* = {valor:.3f}")  # Muestra el estado y su valor.
            if i >= max_mostrar - 1:  # Si se alcanza el l칤mite de estados a mostrar:
                break  # Termina el bucle.

        print("\n游댳 Pol칤tica 칍ptima (*):")
        for i, (estado, accion) in enumerate(self.politica.items()):  # Recorre los estados y acciones.
            print(f"Estado {i+1}: {estado} -> Mejor Acci칩n: {accion}")  # Muestra el estado y su mejor acci칩n.
            if i >= max_mostrar - 1:  # Si se alcanza el l칤mite de pol칤ticas a mostrar:
                break  # Termina el bucle.