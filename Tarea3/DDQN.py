from minatar import Environment
import numpy as np

# ğŸ”¹ Inicializar el entorno de Breakout en MinAtar
entorno_juego = Environment("breakout")  # Cargar el entorno
num_acciones_posibles = entorno_juego.num_actions()  # NÃºmero de acciones posibles
forma_estado_juego = entorno_juego.state_shape()  # Forma del estado
dimension_estado = np.prod(forma_estado_juego)  # DimensiÃ³n total del estado si se aplana

# ğŸ”¹ Imprimir informaciÃ³n del entorno
print(f"NÃºmero de acciones posibles: {num_acciones_posibles}")
print(f"Forma del estado del juego: {forma_estado_juego}")

# ğŸ”¹ Obtener el estado inicial
estado_actual = entorno_juego.state()
print(f"DimensiÃ³n del estado inicial: {estado_actual.shape}")

# ğŸ”¹ Realizar una acciÃ³n aleatoria en el entorno
entorno_juego.reset()  # Reinicia el entorno
accion_ejecutada = np.random.randint(num_acciones_posibles)  # Selecciona una acciÃ³n aleatoria
recompensa_obtenida, episodio_terminado = entorno_juego.act(accion_ejecutada)  # Aplica la acciÃ³n

print(f"AcciÃ³n ejecutada: {accion_ejecutada}, Recompensa: {recompensa_obtenida}, Â¿Episodio terminado? {episodio_terminado}")




import torch  # Biblioteca base de PyTorch
import torch.nn as nn  # MÃ³dulos para definir redes neuronales
import torch.optim as optim  # OptimizaciÃ³n para entrenar la red

# ğŸ”¹ Definir la red neuronal para Deep Q-Network (DQN)
class RedQ(nn.Module):  # La clase hereda de nn.Module (base de redes en PyTorch)
    def __init__(self, forma_entrada, num_acciones):
        super(RedQ, self).__init__()  # Llama al constructor de nn.Module

        # ğŸ”¹ Capa convolucional 1: Detecta patrones espaciales en la imagen
        self.capa_conv1 = nn.Conv2d(
            in_channels=forma_entrada[2],  # NÃºmero de canales de entrada (4 en MinAtar), recuerda; forma_entrada = [10,10,4]
            out_channels=32,  # NÃºmero de filtros aprendidos (32 detectores de caracterÃ­sticas)
            kernel_size=3,  # TamaÃ±o del filtro 3x3
            stride=1,  # Paso del filtro (1 = se mueve un pÃ­xel a la vez)
            padding=1  # Se aÃ±aden ceros alrededor para mantener el tamaÃ±o de salida
        )
        self.activacion1 = nn.ReLU()  # ActivaciÃ³n ReLU para introducir no linealidad

        # ğŸ”¹ Capa convolucional 2: Extrae caracterÃ­sticas mÃ¡s profundas
        self.capa_conv2 = nn.Conv2d(
            in_channels=32,  # Usa las 32 caracterÃ­sticas detectadas en la capa anterior
            out_channels=64,  # Ahora detecta 64 patrones diferentes
            kernel_size=3,
            stride=1,
            padding=1
        )
        self.activacion2 = nn.ReLU()

        # ğŸ”¹ Capa completamente conectada (Dense)
        self.aplanado = nn.Flatten()  # Aplana la salida de las capas convolucionales

        # ğŸ”¹ Primera capa densa: Reduce la dimensionalidad antes de la salida final
        self.capa_densa1 = nn.Linear(
            64 * forma_entrada[0] * forma_entrada[1],  # NÃºmero de entradas (64 mapas de caracterÃ­sticas de 10x10)
            256  # NÃºmero de neuronas ocultas
        )
        self.activacion3 = nn.ReLU()

        # ğŸ”¹ Capa de salida: Predice valores Q para cada acciÃ³n posible
        self.capa_salida = nn.Linear(256, num_acciones)  # La salida tiene el tamaÃ±o de las acciones posibles

    # ğŸ”¹ MÃ©todo para la propagaciÃ³n hacia adelante (cÃ³mo fluye la informaciÃ³n en la red)
    def forward(self, x):
        # âš ï¸ PyTorch espera entradas en formato (batch, canales, alto, ancho)
        # âš ï¸ MinAtar devuelve (batch, alto, ancho, canales), por eso hacemos:
        x = x.permute(0, 3, 1, 2)  # Reorganiza los ejes: (batch, 10, 10, 4) â†’ (batch, 4, 10, 10)
        
        x = self.activacion1(self.capa_conv1(x))  # Paso por la primera convoluciÃ³n y activaciÃ³n
        x = self.activacion2(self.capa_conv2(x))  # Paso por la segunda convoluciÃ³n y activaciÃ³n
        
        x = self.aplanado(x)  # Aplanar la salida antes de pasar a capas densas
        x = self.activacion3(self.capa_densa1(x))  # Paso por la primera capa densa
        
        return self.capa_salida(x)  # Devuelve los valores Q para cada acciÃ³n
    
    #  Entrada: Estado del juego (10x10x4)
    #    â¬‡ï¸  (permute)
    # ReorganizaciÃ³n: (4, 10, 10)
    #    â¬‡ï¸  (capa_conv1)
    # ConvoluciÃ³n 1: 32 filtros â†’ (32, 10, 10)
    #    â¬‡ï¸  (capa_conv2)
    # ConvoluciÃ³n 2: 64 filtros â†’ (64, 10, 10)
    #    â¬‡ï¸  (aplanado)
    # Aplanado: 6400 valores
    #    â¬‡ï¸  (capa_densa1)
    # Capa densa: 256 valores
    #    â¬‡ï¸  (capa_salida)
    # Capa de salida: 6 valores Q (uno por acciÃ³n)


# ğŸ”¹ ConfiguraciÃ³n del dispositivo (usa GPU si estÃ¡ disponible)
dispositivo = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ğŸ”¹ Crear la red principal Q-Network
red_principal = RedQ(forma_entrada=[10, 10, 4], num_acciones=num_acciones_posibles).to(dispositivo)

# ğŸ”¹ Crear la red objetivo Target Q-Network (igual a la red principal inicialmente)
red_objetivo = RedQ(forma_entrada=[10, 10, 4], num_acciones=num_acciones_posibles).to(dispositivo)
red_objetivo.load_state_dict(red_principal.state_dict())  # Copia los pesos de la red principal a la red objetivo
red_objetivo.eval()  # La red objetivo no se entrena directamente

# ğŸ”¹ Definir el optimizador Adam para actualizar la red principal
optimizador = optim.Adam(red_principal.parameters(), lr=0.001)  # Tasa de aprendizaje de 0.001

# ğŸ”¹ Definir la funciÃ³n de pÃ©rdida (MSE: Error cuadrÃ¡tico medio)
funcion_perdida = nn.MSELoss()  # Se usa para comparar los valores Q estimados con los valores objetivo

print("Modelo creado y listo para entrenar.")
