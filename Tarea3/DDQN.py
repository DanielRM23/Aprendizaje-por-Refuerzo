# ðŸ”¹ ImportaciÃ³n de librerÃ­as
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from minatar import Environment

# ===================================================================
# ðŸ”¹ CLASE: Red Neuronal para Deep Q-Network (DQN)
# ===================================================================
class RedQ(nn.Module):  # La clase hereda de nn.Module (base de redes en PyTorch)
    def __init__(self, forma_entrada, num_acciones):
        """
        Inicializa la red neuronal para aproximar la funciÃ³n Q.

        ParÃ¡metros:
        - forma_entrada: DimensiÃ³n del estado del juego (ej. [10,10,4])
        - num_acciones: Cantidad de acciones posibles en el entorno
        """
        super(RedQ, self).__init__()

        # ðŸ”¹ Capa convolucional 1: Detecta patrones espaciales en la imagen
        self.capa_conv1 = nn.Conv2d(in_channels=forma_entrada[2], out_channels=32, kernel_size=3, stride=1, padding=1)
        self.activacion1 = nn.ReLU()  # Introduce no linealidad

        # ðŸ”¹ Capa convolucional 2: Extrae caracterÃ­sticas mÃ¡s profundas
        self.capa_conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.activacion2 = nn.ReLU()

        # ðŸ”¹ Aplanado para convertir en vector
        self.aplanado = nn.Flatten()

        # ðŸ”¹ Primera capa densa: Reduce dimensionalidad
        self.capa_densa1 = nn.Linear(64 * forma_entrada[0] * forma_entrada[1], 256)
        self.activacion3 = nn.ReLU()

        # ðŸ”¹ Capa de salida: Predice valores Q para cada acciÃ³n posible
        self.capa_salida = nn.Linear(256, num_acciones)

    # ðŸ”¹ MÃ©todo para la propagaciÃ³n hacia adelante (cÃ³mo fluye la informaciÃ³n en la red)
    def forward(self, x):
        # âš ï¸ PyTorch espera entradas en formato (batch, canales, alto, ancho)
        # âš ï¸ MinAtar devuelve (batch, alto, ancho, canales), por eso hacemos:
        x = x.permute(0, 3, 1, 2)  # Reorganiza los ejes: (batch, 10, 10, 4) â†’ (batch, 4, 10, 10)
        
        x = self.activacion1(self.capa_conv1(x))  # Paso por la primera convoluciÃ³n y activaciÃ³n
        x = self.activacion2(self.capa_conv2(x))  # Paso por la segunda convoluciÃ³n y activaciÃ³n
        
        x = self.aplanado(x)  # Aplanar la salida antes de pasar a capas densas
        x = self.activacion3(self.capa_densa1(x))  # Paso por la primera capa densa
        
        return self.capa_salida(x)  # Devuelve los valores Q para cada acciÃ³n

    #  Entrada: Estado del juego (10x10x4)
    #    â¬‡ï¸  (permute)
    # ReorganizaciÃ³n: (4, 10, 10)
    #    â¬‡ï¸  (capa_conv1)
    # ConvoluciÃ³n 1: 32 filtros â†’ (32, 10, 10)
    #    â¬‡ï¸  (capa_conv2)
    # ConvoluciÃ³n 2: 64 filtros â†’ (64, 10, 10)
    #    â¬‡ï¸  (aplanado)
    # Aplanado: 6400 valores
    #    â¬‡ï¸  (capa_densa1)
    # Capa densa: 256 valores
    #    â¬‡ï¸  (capa_salida)
    # Capa de salida: 6 valores Q (uno por acciÃ³n)

# ===================================================================
# ðŸ”¹ CLASE: Buffer de Experiencia (Replay Buffer)
# ===================================================================
class BufferExperiencia:
    """
    Buffer de experiencia para almacenar las interacciones del agente con el entorno.
    """

    def __init__(self, capacidad_maxima):
        self.buffer = deque(maxlen=capacidad_maxima)  # FIFO buffer con tamaÃ±o limitado

    def agregar(self, estado, accion, recompensa, nuevo_estado, terminado):
        """
        Agrega una experiencia al buffer.
        """
        self.buffer.append((estado, accion, recompensa, nuevo_estado, terminado))

    def obtener_muestra(self, tamaÃ±o_lote):
        """
        Devuelve una muestra aleatoria de experiencias del buffer.
        """
        return random.sample(self.buffer, tamaÃ±o_lote) if len(self.buffer) >= tamaÃ±o_lote else list(self.buffer)

    def __len__(self):
        """
        Retorna la cantidad de experiencias almacenadas en el buffer.
        """
        return len(self.buffer)

# ===================================================================
# ðŸ”¹ CONFIGURACIÃ“N DEL ENTORNO (MinAtar - Breakout)
# ===================================================================
entorno_juego = Environment("breakout")
num_acciones_posibles = entorno_juego.num_actions()
forma_estado_juego = entorno_juego.state_shape()
dimension_estado = np.prod(forma_estado_juego)

# ðŸ”¹ Imprimir informaciÃ³n del entorno
print(f"NÃºmero de acciones posibles: {num_acciones_posibles}")
print(f"Forma del estado del juego: {forma_estado_juego}")

# ðŸ”¹ Obtener el estado inicial
estado_actual = entorno_juego.state()
print(f"DimensiÃ³n del estado inicial: {estado_actual.shape}")

# ===================================================================
# ðŸ”¹ CONFIGURACIÃ“N DEL MODELO Y BUFFER
# ===================================================================
dispositivo = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ðŸ”¹ Crear la red principal y la red objetivo
red_principal = RedQ(forma_entrada=forma_estado_juego, num_acciones=num_acciones_posibles).to(dispositivo)
red_objetivo = RedQ(forma_entrada=forma_estado_juego, num_acciones=num_acciones_posibles).to(dispositivo)
red_objetivo.load_state_dict(red_principal.state_dict())  # Copiar pesos de la red principal
red_objetivo.eval()  # La red objetivo no se entrena directamente

# ðŸ”¹ Configurar el optimizador y la funciÃ³n de pÃ©rdida
optimizador = optim.Adam(red_principal.parameters(), lr=0.001)
funcion_perdida = nn.MSELoss()

# ðŸ”¹ Inicializar el buffer de experiencia
buffer_replay = BufferExperiencia(capacidad_maxima=100000)

print("Modelo y buffer de experiencia creados. Listos para entrenar.")

# ===================================================================
# ðŸ”¹ PRUEBA DEL BUFFER DE EXPERIENCIA
# ===================================================================
# ðŸ”¹ Simular 15 experiencias y agregarlas al buffer
for i in range(15):
    estado_falso = np.random.rand(10, 10, 4)  # Estado aleatorio de forma (10,10,4)
    accion_falsa = np.random.randint(6)  # AcciÃ³n aleatoria entre 0 y 5
    recompensa_falsa = np.random.random()  # Recompensa aleatoria entre 0 y 1
    nuevo_estado_falso = np.random.rand(10, 10, 4)  # Nuevo estado aleatorio
    terminado_falso = random.choice([True, False])  # Aleatoriamente True o False

    buffer_replay.agregar(estado_falso, accion_falsa, recompensa_falsa, nuevo_estado_falso, terminado_falso)
    
    # ðŸ”¹ Imprimir informaciÃ³n del buffer despuÃ©s de cada inserciÃ³n
    print(f"Experiencia {i+1} agregada. TamaÃ±o actual del buffer: {len(buffer_replay)}")

# ðŸ”¹ Obtener una muestra aleatoria de 5 experiencias
muestra = buffer_replay.obtener_muestra(5)

# ðŸ”¹ Mostrar el contenido de una experiencia de la muestra
print("\nEjemplo de experiencia extraÃ­da del buffer:")
estado_ejemplo, accion_ejemplo, recompensa_ejemplo, nuevo_estado_ejemplo, terminado_ejemplo = muestra[0]

print(f"AcciÃ³n tomada: {accion_ejemplo}")
print(f"Recompensa recibida: {recompensa_ejemplo}")
print(f"Episodio terminado: {terminado_ejemplo}")
print(f"Forma del estado: {estado_ejemplo.shape}")


import numpy as np

class PoliticaEpsilonGreedy:
    """
    Implementa la estrategia Îµ-greedy para la selecciÃ³n de acciones en DDQN.
    """

    def __init__(self, num_acciones, epsilon_inicial=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        """
        Inicializa la polÃ­tica Îµ-greedy.

        ParÃ¡metros:
        - num_acciones: NÃºmero total de acciones posibles en el entorno.
        - epsilon_inicial: Valor inicial de Îµ (alta exploraciÃ³n).
        - epsilon_min: Valor mÃ­nimo de Îµ (cuando el agente ya ha aprendido lo suficiente).
        - epsilon_decay: Factor de decaimiento de Îµ (reduce la exploraciÃ³n gradualmente).
        """
        self.num_acciones = num_acciones
        self.epsilon = epsilon_inicial  # ExploraciÃ³n inicial alta
        self.epsilon_min = epsilon_min  # ExploraciÃ³n mÃ­nima al final del entrenamiento
        self.epsilon_decay = epsilon_decay  # Factor de reducciÃ³n de Îµ en cada episodio

    def seleccionar_accion(self, estado, red_q, dispositivo):
        """
        Selecciona una acciÃ³n usando la estrategia Îµ-greedy.

        ParÃ¡metros:
        - estado: Estado actual del entorno (formato (10,10,4)).
        - red_q: Red neuronal principal que estima valores Q.
        - dispositivo: GPU o CPU donde se ejecuta el modelo.

        Retorna:
        - acciÃ³n seleccionada (int)
        """
        if np.random.rand() < self.epsilon:
            # ðŸ”¹ ExploraciÃ³n: Elegir acciÃ³n aleatoria
            return np.random.randint(self.num_acciones)
        else:
            # ðŸ”¹ ExplotaciÃ³n: Elegir la mejor acciÃ³n segÃºn la red Q
            estado_tensor = torch.tensor(estado, dtype=torch.float32, device=dispositivo).unsqueeze(0)  
            with torch.no_grad():  # No necesitamos gradientes aquÃ­
                valores_q = red_q(estado_tensor)
            return torch.argmax(valores_q).item()  # Retorna la acciÃ³n con el mayor valor Q

    def actualizar_epsilon(self):
        """
        Reduce el valor de Îµ despuÃ©s de cada episodio.
        """
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)  # Nunca baja de Îµ_min


# ðŸ”¹ Inicializar la polÃ­tica Îµ-greedy
politica = PoliticaEpsilonGreedy(num_acciones=num_acciones_posibles)

# ðŸ”¹ Obtener una acciÃ³n con la estrategia Îµ-greedy
accion = politica.seleccionar_accion(estado_actual, red_principal, dispositivo)
print(f"AcciÃ³n seleccionada por Îµ-greedy: {accion}")

# ðŸ”¹ Simular la actualizaciÃ³n de Îµ despuÃ©s de 5 episodios
for episodio in range(5):
    politica.actualizar_epsilon()
    print(f"Episodio {episodio+1}: Îµ = {politica.epsilon}")



import torch


def actualizar_q_values(red_principal, red_objetivo, buffer_replay, optimizador, funcion_perdida, dispositivo, gamma=0.99, batch_size=32):
    """
    Actualiza la red principal utilizando DDQN.
    """
    if len(buffer_replay) < batch_size:
        return  # Esperar mÃ¡s experiencias

    batch = buffer_replay.obtener_muestra(batch_size)

    # ðŸ”¹ Filtrar estados None antes de procesarlos
    batch = [exp for exp in batch if exp[0] is not None and exp[3] is not None]

    # Si despuÃ©s de filtrar hay menos datos de los necesarios, no actualizar
    if len(batch) < batch_size:
        return  

    estados, acciones, recompensas, nuevos_estados, terminados = zip(*batch)

    # ðŸ”¹ Revisar que no haya `None` en los estados antes de transformarlos en tensores
    assert all(s is not None for s in estados), "Hay un estado None en el batch"
    assert all(s is not None for s in nuevos_estados), "Hay un nuevo_estado None en el batch"

    estados = torch.tensor(np.stack(estados, axis=0), dtype=torch.float32, device=dispositivo)
    nuevos_estados = torch.tensor(np.stack(nuevos_estados, axis=0), dtype=torch.float32, device=dispositivo)

    acciones = torch.tensor(acciones, dtype=torch.int64, device=dispositivo).unsqueeze(1)
    recompensas = torch.tensor(recompensas, dtype=torch.float32, device=dispositivo).unsqueeze(1)
    terminados = torch.tensor(terminados, dtype=torch.float32, device=dispositivo).unsqueeze(1)

    # ðŸ”¹ Calcular los valores Q actuales
    valores_q_actuales = red_principal(estados).gather(1, acciones)

    with torch.no_grad():
        mejores_acciones = red_principal(nuevos_estados).argmax(dim=1, keepdim=True)
        valores_q_futuros = red_objetivo(nuevos_estados).gather(1, mejores_acciones)
        q_objetivo = recompensas + gamma * valores_q_futuros * (1 - terminados)

    # ðŸ”¹ Calcular pÃ©rdida y actualizar pesos
    perdida = funcion_perdida(valores_q_actuales, q_objetivo)
    optimizador.zero_grad()
    perdida.backward()
    optimizador.step()




import time

# ðŸ”¹ ParÃ¡metros de entrenamiento
num_episodios = 500  # NÃºmero total de episodios de entrenamiento
gamma = 0.99  # Factor de descuento para recompensas futuras
batch_size = 32  # TamaÃ±o del lote para entrenar la red
actualizar_red_objetivo_cada = 1000  # Cada cuÃ¡ntos pasos copiamos `red_principal` â†’ `red_objetivo`

# ðŸ”¹ Inicializar la polÃ­tica Îµ-greedy
politica = PoliticaEpsilonGreedy(num_acciones=num_acciones_posibles)

# ðŸ”¹ Contador de pasos totales
pasos_totales = 0


import time

# ðŸ”¹ ParÃ¡metros de entrenamiento
num_episodios = 500  # NÃºmero total de episodios de entrenamiento
gamma = 0.99  # Factor de descuento para recompensas futuras
batch_size = 32  # TamaÃ±o del lote para entrenar la red
actualizar_red_objetivo_cada = 1000  # Cada cuÃ¡ntos pasos copiamos `red_principal` â†’ `red_objetivo`

# ðŸ”¹ Inicializar la polÃ­tica Îµ-greedy
politica = PoliticaEpsilonGreedy(num_acciones=num_acciones_posibles)

# ðŸ”¹ Contador de pasos totales
pasos_totales = 0

# ðŸ”¹ Ciclo de entrenamiento
for episodio in range(1, num_episodios + 1):
    # ðŸ”¹ Reiniciar el entorno correctamente
    entorno_juego.reset()  # Resetear el entorno sin asignarlo a una variable
    estado_actual = entorno_juego.state()  # Obtener el estado real despuÃ©s del reset

    # ðŸ”¹ Verificar si `estado_actual` es None y reintentar si es necesario
    intentos_reset = 0
    while estado_actual is None and intentos_reset < 10:  # Limitar intentos para evitar bucles infinitos
        print(f"âš ï¸ Advertencia: `state()` devolviÃ³ None despuÃ©s de reset(). Intento {intentos_reset+1}/10...")
        entorno_juego.reset()
        estado_actual = entorno_juego.state()
        intentos_reset += 1

    if estado_actual is None:
        print("âŒ Error crÃ­tico: No se pudo obtener un estado vÃ¡lido despuÃ©s de 10 intentos. Abortando episodio.")
        continue  # Saltar al siguiente episodio

    recompensa_total = 0  # Acumular la recompensa total del episodio
    terminado = False

    while not terminado:
        # ðŸ”¹ Asegurar que `estado_actual` no sea None antes de seleccionar la acciÃ³n
        if estado_actual is None:
            print("âš ï¸ Advertencia: `estado_actual` es None dentro del episodio. Saliendo del loop...")
            break  # Salir del episodio si el estado es invÃ¡lido

        # ðŸ”¹ Seleccionar acciÃ³n con Îµ-greedy
        accion = politica.seleccionar_accion(estado_actual, red_principal, dispositivo)

        # ðŸ”¹ Ejecutar la acciÃ³n en el entorno
        recompensa, terminado = entorno_juego.act(accion)
        nuevo_estado = entorno_juego.state()

        # ðŸ”¹ Filtrar estados `None` antes de agregar al buffer
        if estado_actual is not None and nuevo_estado is not None:
            buffer_replay.agregar(estado_actual, accion, recompensa, nuevo_estado, terminado)

        # ðŸ”¹ Actualizar la red neuronal si hay suficientes datos
        actualizar_q_values(red_principal, red_objetivo, buffer_replay, optimizador, funcion_perdida, dispositivo, gamma, batch_size)

        # ðŸ”¹ Mover al nuevo estado
        estado_actual = nuevo_estado
        recompensa_total += recompensa
        pasos_totales += 1

        # ðŸ”¹ Actualizar la red objetivo cada `actualizar_red_objetivo_cada` pasos
        if pasos_totales % actualizar_red_objetivo_cada == 0:
            red_objetivo.load_state_dict(red_principal.state_dict())

    # ðŸ”¹ Reducir Îµ despuÃ©s de cada episodio
    politica.actualizar_epsilon()

    # ðŸ”¹ Imprimir estadÃ­sticas del episodio
    print(f"Episodio {episodio}/{num_episodios} - Recompensa total: {recompensa_total:.2f} - Îµ: {politica.epsilon:.4f}")

    # ðŸ”¹ PequeÃ±a pausa para evitar sobrecargar la CPU/GPU
    time.sleep(0.01)

print("Entrenamiento completado ðŸŽ‰")


import matplotlib.pyplot as plt

# ðŸ”¹ Lista para almacenar las recompensas de cada episodio
recompensas_totales = []

# ðŸ”¹ Ciclo de entrenamiento
for episodio in range(1, num_episodios + 1):
    entorno_juego.reset()
    estado_actual = entorno_juego.state()
    
    intentos_reset = 0
    while estado_actual is None and intentos_reset < 10:
        estado_actual = entorno_juego.reset()
        estado_actual = entorno_juego.state()
        intentos_reset += 1

    if estado_actual is None:
        continue

    recompensa_total = 0
    terminado = False

    while not terminado:
        accion = politica.seleccionar_accion(estado_actual, red_principal, dispositivo)
        recompensa, terminado = entorno_juego.act(accion)
        nuevo_estado = entorno_juego.state()

        if estado_actual is not None and nuevo_estado is not None:
            buffer_replay.agregar(estado_actual, accion, recompensa, nuevo_estado, terminado)

        actualizar_q_values(red_principal, red_objetivo, buffer_replay, optimizador, funcion_perdida, dispositivo, gamma, batch_size)

        estado_actual = nuevo_estado
        recompensa_total += recompensa
        pasos_totales += 1

        if pasos_totales % actualizar_red_objetivo_cada == 0:
            red_objetivo.load_state_dict(red_principal.state_dict())

    politica.actualizar_epsilon()
    recompensas_totales.append(recompensa_total)

    print(f"Episodio {episodio}/{num_episodios} - Recompensa total: {recompensa_total:.2f} - Îµ: {politica.epsilon:.4f}")

    time.sleep(0.01)

# ðŸ”¹ Graficar la recompensa total por episodio
plt.plot(recompensas_totales)
plt.xlabel("Episodio")
plt.ylabel("Recompensa Total")
plt.title("DesempeÃ±o del Agente (Recompensa por Episodio)")
plt.show()

