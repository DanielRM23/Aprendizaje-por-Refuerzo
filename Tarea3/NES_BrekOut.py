#  Importaci贸n de librer铆as
import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from minatar import Environment


# ===================================================================
#  CONFIGURACIN DEL ENTORNO (MinAtar - Breakout)
# ===================================================================

entorno_juego = Environment("breakout")
num_acciones_posibles = entorno_juego.num_actions()
forma_estado_juego = entorno_juego.state_shape()
dimension_estado = np.prod(forma_estado_juego)

#  Imprimir informaci贸n del entorno
print(f"N煤mero de acciones posibles: {num_acciones_posibles}")
print(f"Forma del estado del juego: {forma_estado_juego}")

#  Obtener el estado inicial
estado_actual = entorno_juego.state()
print(f"Dimensi贸n del estado inicial: {estado_actual.shape}")



import torch
import torch.nn as nn

# Definimos la clase de la red neuronal para la pol铆tica
class PolicyNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()  # Llama al constructor de nn.Module

        # Creamos un modelo secuencial con:
        # - Una capa lineal que va de input_dim a 64 neuronas
        # - Una funci贸n de activaci贸n ReLU
        # - Otra capa lineal que va de 64 neuronas a output_dim (una por acci贸n posible)
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),  # Capa densa de entrada
            nn.ReLU(),                 # Activaci贸n no lineal
            nn.Linear(64, output_dim)  # Capa de salida con tantas neuronas como acciones
        )

    # M茅todo que define c贸mo se pasa un input por la red
    def forward(self, x):
        # Simplemente pasa el input a trav茅s del modelo secuencial
        return self.model(x)

    # M茅todo para elegir una acci贸n dado un estado
    def act(self, state):
        # Aplana el estado (por si viene en forma 2D o 3D)
        # Lo convierte en tensor float32 y le agrega una dimensi贸n para representar un batch (batch_size = 1)
        state = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0)  # shape: (1, input_dim)

        # Obtiene los valores de salida (logits) para cada acci贸n
        logits = self.forward(state)

        # Toma la acci贸n con el valor m谩s alto (greedy)
        action = torch.argmax(logits, dim=1).item()

        # Devuelve la acci贸n como un n煤mero entero
        return action
